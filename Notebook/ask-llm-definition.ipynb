{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### gpt-4 ###################\n",
    "\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "def ask_llm(query: str, k1_context: pd.Series) -> str:\n",
    "    context = f\"\"\"category: {k1_context[\"ProblemCategory\"]}, keyword: {k1_context[\"ProblemKeyword\"]}, content: {k1_context[\"ProblemDescription\"]}, {k1_context[\"Solution\"]}, etc: {k1_context[\"AdditionalInfo\"]}\"\"\"\n",
    "    prompt = promptTemplate_fstring.format(query=query, context=context)\n",
    "    \n",
    "    print('\\nAsking LLM...')\n",
    "\n",
    "    llm = ChatOpenAI(deployment_id=\"d36b7697328746e0\", temperature=0)\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    print(\"[LOG] 답변 생성 완료\")\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### gemini-1.5-flash ###################\n",
    "\n",
    "\n",
    "from gen_ai_hub.proxy.native.google_vertexai.clients import GenerativeModel\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "\n",
    "def ask_llm(query: str, k1_context: pd.Series) -> str:\n",
    "    context = f\"\"\"category: {k1_context[\"ProblemCategory\"]}, keyword: {k1_context[\"ProblemKeyword\"]}, content: {k1_context[\"ProblemDescription\"]}, {k1_context[\"Solution\"]}, etc: {k1_context[\"AdditionalInfo\"]}\"\"\"\n",
    "    prompt = promptTemplate_fstring.format(query=query, context=context)\n",
    "    \n",
    "    print('\\nAsking LLM...')\n",
    "    \n",
    "    MODEL_NAME = \"gemini-1.5-flash\"\n",
    "    GEN_AI_HUB_PROXY_CLIENT = \"gen-ai-hub\"\n",
    "    \n",
    "    model = GenerativeModel(proxy_client=get_proxy_client(GEN_AI_HUB_PROXY_CLIENT), model_name=MODEL_NAME)\n",
    "    \n",
    "    content = [{\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\n",
    "            \"text\": prompt\n",
    "        }]\n",
    "    }]\n",
    "    \n",
    "    response = model.generate_content(content)\n",
    "    \n",
    "    generated_text = response.candidates[0].content.parts[0].text\n",
    "\n",
    "    print(\"[LOG] 답변 생성 완료\")\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### amazon--titan-text-lite ###################\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from gen_ai_hub.proxy.langchain.amazon import ChatBedrock\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def ask_llm(query: str, k1_context: pd.Series) -> str:\n",
    "    context = f\"\"\"category: {k1_context[\"ProblemCategory\"]}, keyword: {k1_context[\"ProblemKeyword\"]}, content: {k1_context[\"ProblemDescription\"]}, {k1_context[\"Solution\"]}, etc: {k1_context[\"AdditionalInfo\"]}\"\"\"\n",
    "    prompt = promptTemplate_fstring.format(query=query, context=context)\n",
    "\n",
    "    print('\\nAsking LLM...')\n",
    "\n",
    "    MODEL_NAME = \"amazon--titan-text-lite\"\n",
    "    GEN_AI_HUB_PROXY_CLIENT = \"gen-ai-hub\"\n",
    "\n",
    "    model = ChatBedrock(proxy_client=get_proxy_client(GEN_AI_HUB_PROXY_CLIENT), model_name=MODEL_NAME)\n",
    "\n",
    "    content = [HumanMessage(content=prompt)]\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    response = loop.run_until_complete(model.ainvoke(content))\n",
    "\n",
    "    print(\"[LOG] 답변 생성 완료\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### anthropic--claude-3.5-sonnet ###################\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from gen_ai_hub.proxy.langchain.amazon import ChatBedrock\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def ask_llm(query: str, k1_context: pd.Series) -> str:\n",
    "    context = f\"\"\"category: {k1_context[\"ProblemCategory\"]}, keyword: {k1_context[\"ProblemKeyword\"]}, content: {k1_context[\"ProblemDescription\"]}, {k1_context[\"Solution\"]}, etc: {k1_context[\"AdditionalInfo\"]}\"\"\"\n",
    "    prompt = promptTemplate_fstring.format(query=query, context=context)\n",
    "\n",
    "    print('\\nAsking LLM...')\n",
    "\n",
    "    MODEL_NAME = \"anthropic--claude-3.5-sonnet\"\n",
    "    GEN_AI_HUB_PROXY_CLIENT = \"gen-ai-hub\"\n",
    "\n",
    "    model = ChatBedrock(proxy_client=get_proxy_client(GEN_AI_HUB_PROXY_CLIENT), model_name=MODEL_NAME)\n",
    "\n",
    "    content = [HumanMessage(content=prompt)]\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    response = loop.run_until_complete(model.ainvoke(content))\n",
    "\n",
    "    print(\"[LOG] 답변 생성 완료\")\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### mistralai--mixtral-8x7b-instruct-v01 ###################\n",
    "\n",
    "\n",
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "def ask_llm(query: str, k1_context: pd.Series) -> str:\n",
    "    context = f\"\"\"category: {k1_context[\"ProblemCategory\"]}, keyword: {k1_context[\"ProblemKeyword\"]}, content: {k1_context[\"ProblemDescription\"]}, {k1_context[\"Solution\"]}, etc: {k1_context[\"AdditionalInfo\"]}\"\"\"\n",
    "    prompt = promptTemplate_fstring.format(query=query, context=context)\n",
    "    \n",
    "    print('\\nAsking LLM...')\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    kwargs = dict(model_name=\"mistralai--mixtral-8x7b-instruct-v01\", messages=messages)\n",
    "\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    \n",
    "    print(\"[LOG] 답변 생성 완료\")\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### meta--llama3-70b-instruct ###################\n",
    "\n",
    "\n",
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "def ask_llm(query: str, k1_context: pd.Series) -> str:\n",
    "    context = f\"\"\"category: {k1_context[\"ProblemCategory\"]}, keyword: {k1_context[\"ProblemKeyword\"]}, content: {k1_context[\"ProblemDescription\"]}, {k1_context[\"Solution\"]}, etc: {k1_context[\"AdditionalInfo\"]}\"\"\"\n",
    "    prompt = promptTemplate_fstring.format(query=query, context=context)\n",
    "    \n",
    "    print('\\nAsking LLM...')\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    kwargs = dict(model_name=\"meta--llama3-70b-instruct\", messages=messages)\n",
    "\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    \n",
    "    print(\"[LOG] 답변 생성 완료\")\n",
    "    return response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
